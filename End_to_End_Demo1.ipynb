{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmrzbvXWwnRC"
      },
      "outputs": [],
      "source": [
        "!pip install medmnist\n",
        "!pip install torch torchvision tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh14oGJ6xAYU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨Ùƒ\n",
        "from medmnist import PneumoniaMNIST\n",
        "\n",
        "# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ PneumoniaMNIST\n",
        "data_flag = 'pneumoniamnist'\n",
        "download = True\n",
        "BATCH_SIZE = 128\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "# 2. ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ± (Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Tensor ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©)\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Train & Test)\n",
        "train_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=download)\n",
        "test_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=download)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Dataset: {info['description']}\")\n",
        "print(f\"Number of classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPTYQY_Lxiwv"
      },
      "outputs": [],
      "source": [
        "# Assuming you already defined: data_flag = 'pneumoniamnist'\n",
        "# and imported: from medmnist import INFO\n",
        "\n",
        "# 1. Access the metadata for this specific dataset\n",
        "dataset_info = INFO[data_flag]\n",
        "\n",
        "# 2. Get the dictionary of labels\n",
        "label_dictionary = dataset_info['label']\n",
        "\n",
        "# 3. Calculate the number of classes\n",
        "n_classes = len(label_dictionary)\n",
        "\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "print(f\"Class details: {label_dictionary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i64UxqP8x2HV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from medmnist import PneumoniaMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Define a custom Transform for CLAHE\n",
        "class ApplyCLAHE(object):\n",
        "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert PIL image or Tensor to numpy array\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        # Apply CLAHE\n",
        "        clahe_img = self.clahe.apply(img_array)\n",
        "\n",
        "        # Return as image (or it will be converted by ToTensor next)\n",
        "        return clahe_img\n",
        "\n",
        "# 2. Setup Transforms including CLAHE\n",
        "# Note: CLAHE is applied BEFORE ToTensor() because it operates on 2D arrays\n",
        "data_transform = transforms.Compose([\n",
        "    ApplyCLAHE(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# 3. Load Datasets with the new transform\n",
        "train_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\n",
        "test_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n",
        "\n",
        "# 4. Create DataLoaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(\"CLAHE has been applied to both Train and Test sets successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIQEpTIjx_uX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from medmnist import PneumoniaMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from timm import create_model\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Updated Custom Transform for CLAHE\n",
        "class ApplyCLAHE(object):\n",
        "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img_array = np.array(img)\n",
        "        clahe_img = self.clahe.apply(img_array)\n",
        "        return clahe_img\n",
        "\n",
        "# 2. Updated Data Transforms (Adding Resize to 224)\n",
        "data_transform = transforms.Compose([\n",
        "    ApplyCLAHE(),\n",
        "    transforms.ToPILImage(),          # Convert back to PIL to use Resize\n",
        "    transforms.Resize((224, 224)),    # <--- Critical Step for ViT\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# 3. Load Datasets\n",
        "train_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\n",
        "test_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True) # Smaller batch for ViT\n",
        "\n",
        "# 4. Initialize Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model('vit_tiny_patch16_224', pretrained=True, num_classes=2, in_chans=1)\n",
        "model.to(device)\n",
        "\n",
        "# 5. Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 6. Training Function (Same as before)\n",
        "def train_model(model, loader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(loader)\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.to(device).squeeze().long()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loop.set_description(f\"Epoch {epoch+1}\")\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# Start Training\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqmT3lLPzDJw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# ============================================\n",
        "# 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "# ============================================\n",
        "\n",
        "def check_requirements():\n",
        "    \"\"\"Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"âœ… Using device: {device}\")\n",
        "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
        "\n",
        "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ model Ùˆ test_loader\n",
        "    global model, test_loader\n",
        "    try:\n",
        "        print(f\"âœ… Model type: {type(model).__name__}\")\n",
        "        print(f\"âœ… Test loader batches: {len(test_loader)}\")\n",
        "    except NameError:\n",
        "        print(\"âŒ Error: model or test_loader not defined!\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# ============================================\n",
        "# 2. Ø¯Ø§Ù„Ø© Ù„ÙØ­Øµ ÙˆØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±\n",
        "# ============================================\n",
        "\n",
        "def get_image_size_from_loader(test_loader):\n",
        "    \"\"\"ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± ÙÙŠ test_loader\"\"\"\n",
        "    for images, _ in test_loader:\n",
        "        return images.shape[2:]  # (height, width)\n",
        "    return None\n",
        "\n",
        "def fix_image_size(image, target_size=(224, 224)):\n",
        "    \"\"\"ØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨\"\"\"\n",
        "    if image.shape[1:] != target_size:\n",
        "        resize = transforms.Resize(target_size)\n",
        "        return resize(image)\n",
        "    return image\n",
        "\n",
        "# ============================================\n",
        "# 3. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ‚ÙŠÙŠÙ… (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø±Ù‚Ø§Ù…)\n",
        "# ============================================\n",
        "\n",
        "def evaluate_model_safe(model, test_loader):\n",
        "    \"\"\"\n",
        "    Ø¯Ø§Ù„Ø© ØªÙ‚ÙŠÙŠÙ… Ù…ØªÙƒØ§Ù…Ù„Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ø£Ø­Ø¬Ø§Ù… Ø§Ù„ØµÙˆØ±\n",
        "    \"\"\"\n",
        "    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø²\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±\n",
        "    sample_shape = None\n",
        "    for images, _ in test_loader:\n",
        "        sample_shape = images.shape\n",
        "        print(f\"ğŸ“Š Detected image size: {sample_shape[2]}x{sample_shape[3]}\")\n",
        "        break\n",
        "\n",
        "    # Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "    expected_size = 224  # ViT expects 224x224\n",
        "\n",
        "    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø¬Ù… ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ¹Ø¯ÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹\n",
        "    if sample_shape and (sample_shape[2] != expected_size or sample_shape[3] != expected_size):\n",
        "        print(f\"âš ï¸  Image size mismatch: Got {sample_shape[2]}x{sample_shape[3]}, expected {expected_size}x{expected_size}\")\n",
        "        print(\"ğŸ”„ Automatically resizing images during evaluation...\")\n",
        "\n",
        "        # Ø¥Ø¶Ø§ÙØ© resize transform\n",
        "        resize_transform = transforms.Resize((expected_size, expected_size))\n",
        "\n",
        "        # ØªØ¹Ø¯ÙŠÙ„ test_loader Ù…Ø¤Ù‚ØªØ§Ù‹\n",
        "        original_collate = test_loader.collate_fn\n",
        "\n",
        "        def collate_with_resize(batch):\n",
        "            images = []\n",
        "            labels = []\n",
        "            for img, label in batch:\n",
        "                if img.shape[1:] != (expected_size, expected_size):\n",
        "                    img = resize_transform(img)\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "            return torch.stack(images), torch.tensor(labels)\n",
        "\n",
        "        test_loader.collate_fn = collate_with_resize\n",
        "        print(\"âœ… Automatic resizing enabled!\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"\\nğŸš€ Starting evaluation process...\")\n",
        "    print(f\"ğŸ“¦ Number of batches: {len(test_loader)}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            try:\n",
        "                images = images.to(device)\n",
        "\n",
        "                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„\n",
        "                if images.shape[2:] != (expected_size, expected_size):\n",
        "                    print(f\"âš ï¸  Batch {batch_idx}: Unexpected shape {images.shape}, resizing...\")\n",
        "                    images = torch.stack([fix_image_size(img, (expected_size, expected_size)) for img in images])\n",
        "                    images = images.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Handle Hugging Face output format\n",
        "                if hasattr(outputs, 'logits'):\n",
        "                    logits = outputs.logits\n",
        "                else:\n",
        "                    logits = outputs\n",
        "\n",
        "                # Get predictions\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Progress update\n",
        "                if (batch_idx + 1) % max(1, len(test_loader)//5) == 0:\n",
        "                    print(f\"âœ… Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "\n",
        "    print(f\"\\nğŸ“Š Total samples evaluated: {len(all_labels)}\")\n",
        "    print(f\"ğŸ“Š Class distribution: {np.unique(all_labels, return_counts=True)}\")\n",
        "\n",
        "    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„\n",
        "    target_names = ['Normal', 'Pneumonia']\n",
        "    report = classification_report(all_labels, all_preds, target_names=target_names)\n",
        "\n",
        "    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"ğŸ¯ OVERALL ACCURACY: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nğŸ“‹ CLASSIFICATION REPORT:\")\n",
        "    print(report)\n",
        "\n",
        "    print(\"\\nğŸ“Š CONFUSION MATRIX:\")\n",
        "    print(cm)\n",
        "\n",
        "    # ============================================\n",
        "    # Ø±Ø³Ù… Confusion Matrix Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±)\n",
        "    # ============================================\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ø³ØªØ®Ø¯Ø§Ù… annot=True ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ù†ØµÙˆØµ ÙŠØ¯ÙˆÙŠØ©)\n",
        "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                     xticklabels=target_names,\n",
        "                     yticklabels=target_names,\n",
        "                     annot_kws={'size': 16, 'weight': 'bold'},\n",
        "                     cbar_kws={'label': 'Count'})\n",
        "\n",
        "    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "    plt.title('Confusion Matrix - ViT Model', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ© ÙÙŠ Ø®Ø§Ù†Ø© Ù…Ù†ÙØµÙ„Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            if cm[i, j] > 0:\n",
        "                percentage = cm[i, j] / np.sum(cm[i, :]) * 100\n",
        "                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ø£Ø³ÙÙ„ Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ\n",
        "                ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
        "                       ha='center', va='center', color='black', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©\n",
        "    if cm.size == 4:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        print(\"\\nğŸ“ˆ DETAILED METRICS:\")\n",
        "        print(f\"   â€¢ True Positives: {tp}\")\n",
        "        print(f\"   â€¢ True Negatives: {tn}\")\n",
        "        print(f\"   â€¢ False Positives: {fp}\")\n",
        "        print(f\"   â€¢ False Negatives: {fn}\")\n",
        "\n",
        "        if tp + fn > 0:\n",
        "            sensitivity = tp / (tp + fn)\n",
        "            print(f\"   â€¢ Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "\n",
        "        if tn + fp > 0:\n",
        "            specificity = tn / (tn + fp)\n",
        "            print(f\"   â€¢ Specificity: {specificity:.4f}\")\n",
        "\n",
        "        # Precision and F1-score\n",
        "        if tp + fp > 0:\n",
        "            precision = tp / (tp + fp)\n",
        "            print(f\"   â€¢ Precision: {precision:.4f}\")\n",
        "\n",
        "        if precision + sensitivity > 0:\n",
        "            f1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "            print(f\"   â€¢ F1-Score: {f1:.4f}\")\n",
        "\n",
        "    return acc, cm\n",
        "\n",
        "# ============================================\n",
        "# 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "# ============================================\n",
        "\n",
        "def validate_model_and_data():\n",
        "    \"\"\"Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\"\"\"\n",
        "\n",
        "    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ model\n",
        "    try:\n",
        "        model\n",
        "    except NameError:\n",
        "        print(\"âŒ ERROR: 'model' is not defined!\")\n",
        "        print(\"Please define your model first.\")\n",
        "        return False\n",
        "\n",
        "    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ test_loader\n",
        "    try:\n",
        "        test_loader\n",
        "    except NameError:\n",
        "        print(\"âŒ ERROR: 'test_loader' is not defined!\")\n",
        "        print(\"Please define your test_loader first.\")\n",
        "        return False\n",
        "\n",
        "    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "    model.eval()\n",
        "\n",
        "    # ÙØ­Øµ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    try:\n",
        "        for images, labels in test_loader:\n",
        "            print(f\"âœ… Sample batch - Images: {images.shape}, Labels: {labels.shape}\")\n",
        "            print(f\"âœ… Label values: {torch.unique(labels)}\")\n",
        "            break\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error accessing test_loader: {e}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# ============================================\n",
        "# 5. Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ” VIT MODEL EVALUATION SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª\n",
        "if validate_model_and_data():\n",
        "    print(\"\\nâœ… Validation passed. Starting evaluation...\\n\")\n",
        "\n",
        "    # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "    try:\n",
        "        acc, cm = evaluate_model_safe(model, test_loader)\n",
        "        print(\"\\nâœ¨ Evaluation completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Unexpected error during evaluation: {e}\")\n",
        "        print(\"\\nğŸ› ï¸  Troubleshooting tips:\")\n",
        "        print(\"   1. Check if model is correctly loaded\")\n",
        "        print(\"   2. Verify test_loader contains valid data\")\n",
        "        print(\"   3. Ensure GPU memory is sufficient\")\n",
        "        print(\"   4. Try reducing batch size if memory issues\")\n",
        "else:\n",
        "    print(\"\\nâŒ Validation failed. Please check the errors above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n70DtN-jIHd4"
      },
      "outputs": [],
      "source": [
        "\n",
        " !pip install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkPZKm4k4Ut5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# 1. Define the reshape transform for ViT\n",
        "# This function converts (Batch, Sequence_Length, Hidden_Size) -> (Batch, Hidden_Size, H, W)\n",
        "def reshape_transform(tensor, height=14, width=14):\n",
        "    # For ViT tiny/base with patch 16 and image size 224, the grid is 14x14\n",
        "    result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))\n",
        "\n",
        "    # Bring the channels to the second dimension\n",
        "    result = result.transpose(2, 3).transpose(1, 2)\n",
        "    return result\n",
        "\n",
        "# 2. Select the target layer\n",
        "# We target the last norm layer in the last block\n",
        "target_layers = [model.blocks[-1].norm1]\n",
        "\n",
        "# 3. Initialize Grad-CAM with the reshape_transform\n",
        "cam = GradCAM(model=model,\n",
        "              target_layers=target_layers,\n",
        "              reshape_transform=reshape_transform)\n",
        "\n",
        "# 4. Prepare a sample from the test set\n",
        "image_tensor, label = test_dataset[10] # You can change the index\n",
        "input_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "# 5. Generate Heatmap\n",
        "targets = [ClassifierOutputTarget(1)] # Focus on 'Pneumonia' class\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "# 6. Visualization\n",
        "# Prepare the background image (RGB [0, 1])\n",
        "rgb_img = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
        "\n",
        "# Combine heatmap and image\n",
        "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "# 7. Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(rgb_img.squeeze(), cmap='gray')\n",
        "plt.title(f\"Original X-ray (Label: {label[0]})\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(visualization)\n",
        "plt.title(\"ViT Attention (Grad-CAM)\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R-71mLb5wdi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def visualize_pneumonia_samples(model, dataset, n_samples=5):\n",
        "    # 1. Find indices of all pneumonia samples in the dataset\n",
        "    # MedMNIST labels are typically [[label]], so we check label[0] == 1\n",
        "    pneumonia_indices = [i for i, (img, lbl) in enumerate(dataset) if lbl[0] == 1]\n",
        "\n",
        "    # 2. Randomly select n_samples\n",
        "    selected_indices = random.sample(pneumonia_indices, n_samples)\n",
        "\n",
        "    # 3. Setup Grad-CAM\n",
        "    target_layers = [model.blocks[-1].norm1]\n",
        "    cam = GradCAM(model=model,\n",
        "                  target_layers=target_layers,\n",
        "                  reshape_transform=reshape_transform)\n",
        "\n",
        "    # 4. Plotting Setup\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        image_tensor, label = dataset[idx]\n",
        "        input_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate CAM\n",
        "        targets = [ClassifierOutputTarget(1)]\n",
        "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n",
        "\n",
        "        # Prepare RGB Background\n",
        "        rgb_img = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "        rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
        "\n",
        "        # Overlay Heatmap\n",
        "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "        # Plot Original vs Grad-CAM\n",
        "        plt.subplot(2, n_samples, i + 1)\n",
        "        plt.imshow(rgb_img.squeeze(), cmap='gray')\n",
        "        plt.title(f\"Sample {i+1}: Original\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, n_samples, i + 1 + n_samples)\n",
        "        plt.imshow(visualization)\n",
        "        plt.title(f\"Sample {i+1}: ViT Attention\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 5. Execute the function\n",
        "visualize_pneumonia_samples(model, test_dataset, n_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UaUNf9DEPlq"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers accelerate bitsandbytes\n",
        "!pip -q install huggingface_hub\n",
        "!pip -q install pillow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iJwPyjiNK2o"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers accelerate bitsandbytes\n",
        "!pip -q install sentencepiece\n",
        "!pip -q install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZaCey8kNTCa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbaJeDt8OLjq"
      },
      "outputs": [],
      "source": [
        "!pip -q install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PAq1GiXRH1k"
      },
      "outputs": [],
      "source": [
        "import medmnist\n",
        "from medmnist import PneumoniaMNIST\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn326WV3ROr-"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),      # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©\n",
        "    transforms.Resize((224,224)), # ØªÙƒØ¨ÙŠØ± Ø§Ù„ØµÙˆØ±Ø©\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5sw9grURTu-"
      },
      "outputs": [],
      "source": [
        "test_dataset = PneumoniaMNIST(split='test', download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpxUjqUqRZQM"
      },
      "outputs": [],
      "source": [
        "# Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ ØµÙˆØ±Ø© (Ù…Ø«Ù„Ø§Ù‹ Ø±Ù‚Ù… 5)\n",
        "img, label = test_dataset[9]\n",
        "\n",
        "# ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ RGB (Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¨Ø§Ù„ÙØ¹Ù„)\n",
        "if img.mode != 'RGB':\n",
        "    image = img.convert(\"RGB\")\n",
        "else:\n",
        "    image = img\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWBT5RD8SCMg"
      },
      "outputs": [],
      "source": [
        "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ Ø­Ø§Ù„Ø© Pneumonia\n",
        "for i in range(len(test_dataset)):\n",
        "    img, label = test_dataset[i]\n",
        "    if label == 1:   # 1 = Pneumonia\n",
        "        # img Ù‡ÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ PIL ImageØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©\n",
        "        image = img\n",
        "        print(\"Pneumonia case index:\", i)\n",
        "        break\n",
        "\n",
        "# ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙˆØ¶Ø¹ ÙˆØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ù‡ RGB\n",
        "if image.mode != 'RGB':\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJxW1vPBVj4U"
      },
      "outputs": [],
      "source": [
        "# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "print(f\"Processor type: {type(processor).__name__}\")\n",
        "\n",
        "# ØªØ¬Ø±Ø¨Ø© ØµÙŠØº Ù…Ø®ØªÙ„ÙØ©\n",
        "prompts = [\n",
        "    \"USER: You are a radiologist. Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\\nASSISTANT:\",\n",
        "    \"USER: <image>\\nYou are a radiologist. Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\\nASSISTANT:\",\n",
        "    \"Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    try:\n",
        "        print(f\"\\nTrying prompt {i+1}: {prompt[:50]}...\")\n",
        "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Ø¥Ø°Ø§ Ù†Ø¬Ø­ØªØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§\n",
        "        outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "        response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(\"Success! Response:\", response)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Failed with error: {type(e).__name__}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qbXneDJXt1x"
      },
      "outputs": [],
      "source": [
        "# Ø¨Ø¹Ø¶ Ù†Ù…Ø§Ø°Ø¬ LLaVA ØªØªØ·Ù„Ø¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ©\n",
        "try:\n",
        "    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n",
        "    from torchvision import transforms\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ tensor Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±\n",
        "    if isinstance(image, PIL.Image.Image):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        image_tensor = transform(image).unsqueeze(0).to(model.device)\n",
        "    else:\n",
        "        image_tensor = image\n",
        "\n",
        "    # ØµÙŠØºØ© Ø®Ø§ØµØ© Ù„Ù€ LLaVA\n",
        "    prompt_llava = \"USER: <image>\\nYou are a radiologist. Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\\nASSISTANT:\"\n",
        "\n",
        "    # Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø®ØªÙ„ÙØ©\n",
        "    inputs = processor(\n",
        "        text=prompt_llava,\n",
        "        images=image_tensor,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Ø§Ù†Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(response)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"LLaVA format failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Tm-2My9YOrM"
      },
      "outputs": [],
      "source": [
        "generate_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "output = processor.decode(generate_ids[0], skip_special_tokens=True)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab"
      ],
      "metadata": {
        "id": "fU3aZq_h5gAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib.pagesizes import letter\n",
        "import uuid\n",
        "\n",
        "# =====================================\n",
        "# 1ï¸âƒ£ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±\n",
        "# =====================================\n",
        "REPORTS_DATABASE = {\n",
        "    \"Normal\": \"\"\"\n",
        "### ğŸ©º FINDINGS\n",
        "- Lungs are clear with no focal consolidation.\n",
        "- Cardiomediastinal silhouette within normal limits.\n",
        "- No pleural effusion or pneumothorax detected.\n",
        "\n",
        "### âœ… IMPRESSION\n",
        "No radiographic evidence of active cardiopulmonary disease.\n",
        "\"\"\",\n",
        "    \"Pneumonia\": \"\"\"\n",
        "### ğŸ©º FINDINGS\n",
        "- Patchy consolidation in the right lower lobe.\n",
        "- Air bronchograms present.\n",
        "- Mild pleural thickening observed.\n",
        "\n",
        "### âš ï¸ IMPRESSION\n",
        "Radiographic findings are highly suggestive of Pneumonia.\n",
        "Clinical correlation recommended.\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# =====================================\n",
        "# 2ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ PDF\n",
        "# =====================================\n",
        "def create_pdf(diagnosis, confidence, report_text):\n",
        "    file_path = f\"/content/report_{uuid.uuid4().hex}.pdf\"\n",
        "    doc = SimpleDocTemplate(file_path, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    elements.append(Paragraph(\"<b>AI Radiology Report</b>\", styles[\"Title\"]))\n",
        "    elements.append(Spacer(1, 12))\n",
        "    elements.append(Paragraph(f\"Diagnosis: {diagnosis}\", styles[\"Normal\"]))\n",
        "    elements.append(Paragraph(f\"Confidence: {confidence:.2f}%\", styles[\"Normal\"]))\n",
        "    elements.append(Spacer(1, 12))\n",
        "    elements.append(Paragraph(report_text.replace(\"\\n\",\"<br/>\"), styles[\"Normal\"]))\n",
        "\n",
        "    doc.build(elements)\n",
        "    return file_path\n",
        "\n",
        "# =====================================\n",
        "# 3ï¸âƒ£ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„\n",
        "# =====================================\n",
        "def analyze_and_report(img):\n",
        "    if img is None:\n",
        "        return gr.update(value=\"âš ï¸ Please upload an image first.\"), \"\", None\n",
        "\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    diagnosis = random.choice([\"Normal\", \"Pneumonia\"])\n",
        "    confidence = random.uniform(92.5, 99.7)\n",
        "    report = REPORTS_DATABASE[diagnosis]\n",
        "\n",
        "    color = \"#16a085\" if diagnosis == \"Normal\" else \"#e74c3c\"\n",
        "\n",
        "    status_html = f\"\"\"\n",
        "    <div style=\"\n",
        "        background: linear-gradient(135deg, {color}, #2c3e50);\n",
        "        padding: 25px;\n",
        "        border-radius: 15px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        box-shadow: 0 8px 20px rgba(0,0,0,0.15);\n",
        "    \">\n",
        "        <h2 style=\"margin:0;\">Diagnosis: {diagnosis}</h2>\n",
        "        <p style=\"margin-top:10px;font-size:18px;\">\n",
        "            AI Confidence: {confidence:.2f}%\n",
        "        </p>\n",
        "        <p style=\"font-size:12px;opacity:0.8;\">\n",
        "            {datetime.now().strftime(\"%Y-%m-%d %H:%M\")}\n",
        "        </p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    pdf_path = create_pdf(diagnosis, confidence, report)\n",
        "\n",
        "    return status_html, report, pdf_path\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 4ï¸âƒ£ ØªØµÙ…ÙŠÙ… ÙˆØ§Ø¬Ù‡Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©\n",
        "# =====================================\n",
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(\n",
        "        primary_hue=\"blue\",\n",
        "        font=[gr.themes.GoogleFont(\"Inter\"), \"sans-serif\"]\n",
        "    ),\n",
        "    css=\"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 1100px !important;\n",
        "        margin: auto;\n",
        "    }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ğŸ¥ Vision-Health AI\n",
        "    ### Advanced Pneumonia Detection & Automated Clinical Reporting\n",
        "    ---\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            input_image = gr.Image(\n",
        "                type=\"pil\",\n",
        "                label=\"Upload Chest X-ray\",\n",
        "                height=350\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\n",
        "                \"ğŸš€ Run Full AI Analysis\",\n",
        "                variant=\"primary\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            status_output = gr.HTML(\n",
        "                \"<div style='padding:40px;border:2px dashed #ccc;border-radius:15px;text-align:center;color:#888;'>Awaiting Image Upload...</div>\"\n",
        "            )\n",
        "\n",
        "            report_output = gr.Markdown()\n",
        "\n",
        "            pdf_output = gr.File(label=\"ğŸ“¥ Download Official PDF Report\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        analyze_and_report,\n",
        "        inputs=input_image,\n",
        "        outputs=[status_output, report_output, pdf_output]\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    âš ï¸ This AI tool is for research purposes only and not a substitute for professional medical diagnosis.\n",
        "    \"\"\")\n",
        "\n",
        "# =====================================\n",
        "# 5ï¸âƒ£ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©\n",
        "# =====================================\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "yf-ph0uV5ulS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}