{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22906,"status":"ok","timestamp":1771357341829,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"CmrzbvXWwnRC","outputId":"37320d0f-b22f-45ee-855d-9590faa071f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting medmnist\n","  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.6.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from medmnist) (11.3.0)\n","Collecting fire (from medmnist)\n","  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.9.0+cpu)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.24.0+cpu)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.3)\n","Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (1.16.3)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (3.6.1)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2026.1.28)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (26.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.21.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.14.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2025.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->medmnist) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->medmnist) (3.0.3)\n","Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n","Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fire, medmnist\n","Successfully installed fire-0.7.1 medmnist-3.0.2\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.21.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"]}],"source":["!pip install medmnist\n","!pip install torch torchvision tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26569,"status":"ok","timestamp":1771357368442,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"Gh14oGJ6xAYU","outputId":"54cfc44b-7d1c-4648-b697-4a31c4c0d8d6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.17M/4.17M [00:00<00:00, 4.53MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Dataset: The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384âˆ’2,916)Ã—(127âˆ’2,713). We center-crop the images and resize them into 1Ã—28Ã—28.\n","Number of classes: 2\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import medmnist\n","from medmnist import INFO, Evaluator\n","# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨Ùƒ\n","from medmnist import PneumoniaMNIST\n","\n","# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ PneumoniaMNIST\n","data_flag = 'pneumoniamnist'\n","download = True\n","BATCH_SIZE = 128\n","info = INFO[data_flag]\n","n_channels = info['n_channels']\n","n_classes = len(info['label'])\n","\n","# 2. ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ± (Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Tensor ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©)\n","data_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[.5], std=[.5])\n","])\n","\n","# 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Train & Test)\n","train_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=download)\n","test_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=download)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","print(f\"Dataset: {info['description']}\")\n","print(f\"Number of classes: {n_classes}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77,"status":"ok","timestamp":1771357368529,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"sPTYQY_Lxiwv","outputId":"f3e971f3-1883-4639-ca4d-6409f17f920e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of classes: 2\n","Class details: {'0': 'normal', '1': 'pneumonia'}\n"]}],"source":["# Assuming you already defined: data_flag = 'pneumoniamnist'\n","# and imported: from medmnist import INFO\n","\n","# 1. Access the metadata for this specific dataset\n","dataset_info = INFO[data_flag]\n","\n","# 2. Get the dictionary of labels\n","label_dictionary = dataset_info['label']\n","\n","# 3. Calculate the number of classes\n","n_classes = len(label_dictionary)\n","\n","print(f\"Number of classes: {n_classes}\")\n","print(f\"Class details: {label_dictionary}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1771357368670,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"i64UxqP8x2HV","outputId":"ee77f60a-3510-419b-ef3e-2868e41ed525"},"outputs":[{"output_type":"stream","name":"stdout","text":["CLAHE has been applied to both Train and Test sets successfully.\n"]}],"source":["import numpy as np\n","import cv2\n","from torchvision import transforms\n","from medmnist import PneumoniaMNIST\n","from torch.utils.data import DataLoader\n","\n","# 1. Define a custom Transform for CLAHE\n","class ApplyCLAHE(object):\n","    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n","        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n","\n","    def __call__(self, img):\n","        # Convert PIL image or Tensor to numpy array\n","        img_array = np.array(img)\n","\n","        # Apply CLAHE\n","        clahe_img = self.clahe.apply(img_array)\n","\n","        # Return as image (or it will be converted by ToTensor next)\n","        return clahe_img\n","\n","# 2. Setup Transforms including CLAHE\n","# Note: CLAHE is applied BEFORE ToTensor() because it operates on 2D arrays\n","data_transform = transforms.Compose([\n","    ApplyCLAHE(clip_limit=2.0, tile_grid_size=(8, 8)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","# 3. Load Datasets with the new transform\n","train_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\n","test_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n","\n","# 4. Create DataLoaders\n","train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)\n","\n","print(\"CLAHE has been applied to both Train and Test sets successfully.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495,"referenced_widgets":["8f93a62897cb4fca8b44f313d29f73bd","5b6bd0a30ef24c17817a4e1da8608b8e","1f60850a301c4809b9451e7b5a3a33c3","08ad5a410312496b9c1f518a208544ac","4d6c2fc28d854f288b433a41ef3eb72d","2712e63f74904d3e83414a1c44d334e7","1ccc4e4568ee400caaeb54d586c76f9c","ae364c2f2f7c434182cb9a352b91e2d9","8481e09ffee041c484bab4cba9d97e5e","671be8262c904f6a82985d82e3b84e4f","66e1bf6b33c044a7957ac9af777422ee"]},"executionInfo":{"elapsed":63281,"status":"error","timestamp":1771357431962,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"GIQEpTIjx_uX","outputId":"1694ef2d-b3ae-40cb-e6ce-1a2b846944f7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f93a62897cb4fca8b44f313d29f73bd"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Epoch 1:   3%|â–         | 4/148 [00:46<27:39, 11.52s/it, loss=0.445]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-659989229.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Start Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-659989229.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m   1226\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import cv2\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from medmnist import PneumoniaMNIST\n","from torch.utils.data import DataLoader\n","from timm import create_model\n","from tqdm import tqdm\n","\n","# 1. Updated Custom Transform for CLAHE\n","class ApplyCLAHE(object):\n","    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n","        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n","\n","    def __call__(self, img):\n","        img_array = np.array(img)\n","        clahe_img = self.clahe.apply(img_array)\n","        return clahe_img\n","\n","# 2. Updated Data Transforms (Adding Resize to 224)\n","data_transform = transforms.Compose([\n","    ApplyCLAHE(),\n","    transforms.ToPILImage(),          # Convert back to PIL to use Resize\n","    transforms.Resize((224, 224)),    # <--- Critical Step for ViT\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","# 3. Load Datasets\n","train_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\n","test_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True) # Smaller batch for ViT\n","\n","# 4. Initialize Model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = create_model('vit_tiny_patch16_224', pretrained=True, num_classes=2, in_chans=1)\n","model.to(device)\n","\n","# 5. Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# 6. Training Function (Same as before)\n","def train_model(model, loader, criterion, optimizer, epochs=5):\n","    model.train()\n","    for epoch in range(epochs):\n","        loop = tqdm(loader)\n","        for images, labels in loop:\n","            images, labels = images.to(device), labels.to(device).squeeze().long()\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            loop.set_description(f\"Epoch {epoch+1}\")\n","            loop.set_postfix(loss=loss.item())\n","\n","# Start Training\n","train_model(model, train_loader, criterion, optimizer, epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":113767,"status":"aborted","timestamp":1771357431984,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"ZqmT3lLPzDJw"},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# ============================================\n","# 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n","# ============================================\n","\n","def check_requirements():\n","    \"\"\"Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª\"\"\"\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"âœ… Using device: {device}\")\n","    print(f\"âœ… PyTorch version: {torch.__version__}\")\n","\n","    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ model Ùˆ test_loader\n","    global model, test_loader\n","    try:\n","        print(f\"âœ… Model type: {type(model).__name__}\")\n","        print(f\"âœ… Test loader batches: {len(test_loader)}\")\n","    except NameError:\n","        print(\"âŒ Error: model or test_loader not defined!\")\n","        return False\n","\n","    return True\n","\n","# ============================================\n","# 2. Ø¯Ø§Ù„Ø© Ù„ÙØ­Øµ ÙˆØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±\n","# ============================================\n","\n","def get_image_size_from_loader(test_loader):\n","    \"\"\"ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± ÙÙŠ test_loader\"\"\"\n","    for images, _ in test_loader:\n","        return images.shape[2:]  # (height, width)\n","    return None\n","\n","def fix_image_size(image, target_size=(224, 224)):\n","    \"\"\"ØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨\"\"\"\n","    if image.shape[1:] != target_size:\n","        resize = transforms.Resize(target_size)\n","        return resize(image)\n","    return image\n","\n","# ============================================\n","# 3. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ‚ÙŠÙŠÙ… (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø±Ù‚Ø§Ù…)\n","# ============================================\n","\n","def evaluate_model_safe(model, test_loader):\n","    \"\"\"\n","    Ø¯Ø§Ù„Ø© ØªÙ‚ÙŠÙŠÙ… Ù…ØªÙƒØ§Ù…Ù„Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ø£Ø­Ø¬Ø§Ù… Ø§Ù„ØµÙˆØ±\n","    \"\"\"\n","    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø²\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±\n","    sample_shape = None\n","    for images, _ in test_loader:\n","        sample_shape = images.shape\n","        print(f\"ğŸ“Š Detected image size: {sample_shape[2]}x{sample_shape[3]}\")\n","        break\n","\n","    # Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬\n","    expected_size = 224  # ViT expects 224x224\n","\n","    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø¬Ù… ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ¹Ø¯ÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹\n","    if sample_shape and (sample_shape[2] != expected_size or sample_shape[3] != expected_size):\n","        print(f\"âš ï¸  Image size mismatch: Got {sample_shape[2]}x{sample_shape[3]}, expected {expected_size}x{expected_size}\")\n","        print(\"ğŸ”„ Automatically resizing images during evaluation...\")\n","\n","        # Ø¥Ø¶Ø§ÙØ© resize transform\n","        resize_transform = transforms.Resize((expected_size, expected_size))\n","\n","        # ØªØ¹Ø¯ÙŠÙ„ test_loader Ù…Ø¤Ù‚ØªØ§Ù‹\n","        original_collate = test_loader.collate_fn\n","\n","        def collate_with_resize(batch):\n","            images = []\n","            labels = []\n","            for img, label in batch:\n","                if img.shape[1:] != (expected_size, expected_size):\n","                    img = resize_transform(img)\n","                images.append(img)\n","                labels.append(label)\n","            return torch.stack(images), torch.tensor(labels)\n","\n","        test_loader.collate_fn = collate_with_resize\n","        print(\"âœ… Automatic resizing enabled!\")\n","\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    print(\"\\nğŸš€ Starting evaluation process...\")\n","    print(f\"ğŸ“¦ Number of batches: {len(test_loader)}\")\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, labels) in enumerate(test_loader):\n","            try:\n","                images = images.to(device)\n","\n","                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„\n","                if images.shape[2:] != (expected_size, expected_size):\n","                    print(f\"âš ï¸  Batch {batch_idx}: Unexpected shape {images.shape}, resizing...\")\n","                    images = torch.stack([fix_image_size(img, (expected_size, expected_size)) for img in images])\n","                    images = images.to(device)\n","\n","                # Forward pass\n","                outputs = model(images)\n","\n","                # Handle Hugging Face output format\n","                if hasattr(outputs, 'logits'):\n","                    logits = outputs.logits\n","                else:\n","                    logits = outputs\n","\n","                # Get predictions\n","                preds = torch.argmax(logits, dim=1)\n","\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","                # Progress update\n","                if (batch_idx + 1) % max(1, len(test_loader)//5) == 0:\n","                    print(f\"âœ… Processed {batch_idx + 1}/{len(test_loader)} batches\")\n","\n","            except Exception as e:\n","                print(f\"âŒ Error in batch {batch_idx}: {e}\")\n","                continue\n","\n","    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy\n","    all_labels = np.array(all_labels).flatten()\n","    all_preds = np.array(all_preds).flatten()\n","\n","    print(f\"\\nğŸ“Š Total samples evaluated: {len(all_labels)}\")\n","    print(f\"ğŸ“Š Class distribution: {np.unique(all_labels, return_counts=True)}\")\n","\n","    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\n","    acc = accuracy_score(all_labels, all_preds)\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    # Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„\n","    target_names = ['Normal', 'Pneumonia']\n","    report = classification_report(all_labels, all_preds, target_names=target_names)\n","\n","    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n","    print(\"\\n\" + \"=\"*50)\n","    print(f\"ğŸ¯ OVERALL ACCURACY: {acc:.4f} ({acc*100:.2f}%)\")\n","    print(\"=\"*50)\n","    print(\"\\nğŸ“‹ CLASSIFICATION REPORT:\")\n","    print(report)\n","\n","    print(\"\\nğŸ“Š CONFUSION MATRIX:\")\n","    print(cm)\n","\n","    # ============================================\n","    # Ø±Ø³Ù… Confusion Matrix Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±)\n","    # ============================================\n","    plt.figure(figsize=(12, 10))\n","\n","    # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ø³ØªØ®Ø¯Ø§Ù… annot=True ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ù†ØµÙˆØµ ÙŠØ¯ÙˆÙŠØ©)\n","    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                     xticklabels=target_names,\n","                     yticklabels=target_names,\n","                     annot_kws={'size': 16, 'weight': 'bold'},\n","                     cbar_kws={'label': 'Count'})\n","\n","    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n","    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n","    plt.title('Confusion Matrix - ViT Model', fontsize=16, fontweight='bold')\n","\n","    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ© ÙÙŠ Ø®Ø§Ù†Ø© Ù…Ù†ÙØµÙ„Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            if cm[i, j] > 0:\n","                percentage = cm[i, j] / np.sum(cm[i, :]) * 100\n","                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ø£Ø³ÙÙ„ Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ\n","                ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n","                       ha='center', va='center', color='black', fontsize=10)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©\n","    if cm.size == 4:\n","        tn, fp, fn, tp = cm.ravel()\n","\n","        print(\"\\nğŸ“ˆ DETAILED METRICS:\")\n","        print(f\"   â€¢ True Positives: {tp}\")\n","        print(f\"   â€¢ True Negatives: {tn}\")\n","        print(f\"   â€¢ False Positives: {fp}\")\n","        print(f\"   â€¢ False Negatives: {fn}\")\n","\n","        if tp + fn > 0:\n","            sensitivity = tp / (tp + fn)\n","            print(f\"   â€¢ Sensitivity (Recall): {sensitivity:.4f}\")\n","\n","        if tn + fp > 0:\n","            specificity = tn / (tn + fp)\n","            print(f\"   â€¢ Specificity: {specificity:.4f}\")\n","\n","        # Precision and F1-score\n","        if tp + fp > 0:\n","            precision = tp / (tp + fp)\n","            print(f\"   â€¢ Precision: {precision:.4f}\")\n","\n","        if precision + sensitivity > 0:\n","            f1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n","            print(f\"   â€¢ F1-Score: {f1:.4f}\")\n","\n","    return acc, cm\n","\n","# ============================================\n","# 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n","# ============================================\n","\n","def validate_model_and_data():\n","    \"\"\"Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\"\"\"\n","\n","    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ model\n","    try:\n","        model\n","    except NameError:\n","        print(\"âŒ ERROR: 'model' is not defined!\")\n","        print(\"Please define your model first.\")\n","        return False\n","\n","    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ test_loader\n","    try:\n","        test_loader\n","    except NameError:\n","        print(\"âŒ ERROR: 'test_loader' is not defined!\")\n","        print(\"Please define your test_loader first.\")\n","        return False\n","\n","    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n","    model.eval()\n","\n","    # ÙØ­Øµ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n","    try:\n","        for images, labels in test_loader:\n","            print(f\"âœ… Sample batch - Images: {images.shape}, Labels: {labels.shape}\")\n","            print(f\"âœ… Label values: {torch.unique(labels)}\")\n","            break\n","    except Exception as e:\n","        print(f\"âŒ Error accessing test_loader: {e}\")\n","        return False\n","\n","    return True\n","\n","# ============================================\n","# 5. Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\n","# ============================================\n","\n","print(\"=\"*60)\n","print(\"ğŸ” VIT MODEL EVALUATION SYSTEM\")\n","print(\"=\"*60)\n","\n","# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª\n","if validate_model_and_data():\n","    print(\"\\nâœ… Validation passed. Starting evaluation...\\n\")\n","\n","    # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n","    try:\n","        acc, cm = evaluate_model_safe(model, test_loader)\n","        print(\"\\nâœ¨ Evaluation completed successfully!\")\n","    except Exception as e:\n","        print(f\"\\nâŒ Unexpected error during evaluation: {e}\")\n","        print(\"\\nğŸ› ï¸  Troubleshooting tips:\")\n","        print(\"   1. Check if model is correctly loaded\")\n","        print(\"   2. Verify test_loader contains valid data\")\n","        print(\"   3. Ensure GPU memory is sufficient\")\n","        print(\"   4. Try reducing batch size if memory issues\")\n","else:\n","    print(\"\\nâŒ Validation failed. Please check the errors above.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1771357431997,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"n70DtN-jIHd4"},"outputs":[],"source":["\n"," !pip install grad-cam"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":113752,"status":"aborted","timestamp":1771357432009,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"WkPZKm4k4Ut5"},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pytorch_grad_cam import GradCAM\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","\n","# 1. Define the reshape transform for ViT\n","# This function converts (Batch, Sequence_Length, Hidden_Size) -> (Batch, Hidden_Size, H, W)\n","def reshape_transform(tensor, height=14, width=14):\n","    # For ViT tiny/base with patch 16 and image size 224, the grid is 14x14\n","    result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))\n","\n","    # Bring the channels to the second dimension\n","    result = result.transpose(2, 3).transpose(1, 2)\n","    return result\n","\n","# 2. Select the target layer\n","# We target the last norm layer in the last block\n","target_layers = [model.blocks[-1].norm1]\n","\n","# 3. Initialize Grad-CAM with the reshape_transform\n","cam = GradCAM(model=model,\n","              target_layers=target_layers,\n","              reshape_transform=reshape_transform)\n","\n","# 4. Prepare a sample from the test set\n","image_tensor, label = test_dataset[10] # You can change the index\n","input_tensor = image_tensor.unsqueeze(0).to(device)\n","\n","# 5. Generate Heatmap\n","targets = [ClassifierOutputTarget(1)] # Focus on 'Pneumonia' class\n","grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n","grayscale_cam = grayscale_cam[0, :]\n","\n","# 6. Visualization\n","# Prepare the background image (RGB [0, 1])\n","rgb_img = image_tensor.permute(1, 2, 0).cpu().numpy()\n","rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n","\n","# Combine heatmap and image\n","visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n","\n","# 7. Plotting\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.imshow(rgb_img.squeeze(), cmap='gray')\n","plt.title(f\"Original X-ray (Label: {label[0]})\")\n","plt.axis('off')\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(visualization)\n","plt.title(\"ViT Attention (Grad-CAM)\")\n","plt.axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":113737,"status":"aborted","timestamp":1771357432016,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"8R-71mLb5wdi"},"outputs":[],"source":["import random\n","\n","def visualize_pneumonia_samples(model, dataset, n_samples=5):\n","    # 1. Find indices of all pneumonia samples in the dataset\n","    # MedMNIST labels are typically [[label]], so we check label[0] == 1\n","    pneumonia_indices = [i for i, (img, lbl) in enumerate(dataset) if lbl[0] == 1]\n","\n","    # 2. Randomly select n_samples\n","    selected_indices = random.sample(pneumonia_indices, n_samples)\n","\n","    # 3. Setup Grad-CAM\n","    target_layers = [model.blocks[-1].norm1]\n","    cam = GradCAM(model=model,\n","                  target_layers=target_layers,\n","                  reshape_transform=reshape_transform)\n","\n","    # 4. Plotting Setup\n","    plt.figure(figsize=(20, 10))\n","\n","    for i, idx in enumerate(selected_indices):\n","        image_tensor, label = dataset[idx]\n","        input_tensor = image_tensor.unsqueeze(0).to(device)\n","\n","        # Generate CAM\n","        targets = [ClassifierOutputTarget(1)]\n","        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n","\n","        # Prepare RGB Background\n","        rgb_img = image_tensor.permute(1, 2, 0).cpu().numpy()\n","        rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n","\n","        # Overlay Heatmap\n","        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n","\n","        # Plot Original vs Grad-CAM\n","        plt.subplot(2, n_samples, i + 1)\n","        plt.imshow(rgb_img.squeeze(), cmap='gray')\n","        plt.title(f\"Sample {i+1}: Original\")\n","        plt.axis('off')\n","\n","        plt.subplot(2, n_samples, i + 1 + n_samples)\n","        plt.imshow(visualization)\n","        plt.title(f\"Sample {i+1}: ViT Attention\")\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# 5. Execute the function\n","visualize_pneumonia_samples(model, test_dataset, n_samples=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":113726,"status":"aborted","timestamp":1771357432028,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"},"user_tz":-180},"id":"7UaUNf9DEPlq"},"outputs":[],"source":["!pip -q install transformers accelerate bitsandbytes\n","!pip -q install huggingface_hub\n","!pip -q install pillow matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4iJwPyjiNK2o","executionInfo":{"status":"aborted","timestamp":1771357432036,"user_tz":-180,"elapsed":113713,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["!pip -q install transformers accelerate bitsandbytes\n","!pip -q install sentencepiece\n","!pip -q install pillow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZaCey8kNTCa","executionInfo":{"status":"aborted","timestamp":1771357432088,"user_tz":-180,"elapsed":113747,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["import torch\n","from transformers import LlavaForConditionalGeneration, AutoProcessor\n","\n","model_id = \"llava-hf/llava-1.5-7b-hf\"\n","\n","processor = AutoProcessor.from_pretrained(model_id)\n","\n","model = LlavaForConditionalGeneration.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbaJeDt8OLjq","executionInfo":{"status":"aborted","timestamp":1771357432110,"user_tz":-180,"elapsed":13,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["!pip -q install medmnist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PAq1GiXRH1k","executionInfo":{"status":"aborted","timestamp":1771357432118,"user_tz":-180,"elapsed":13,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["import medmnist\n","from medmnist import PneumoniaMNIST\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bn326WV3ROr-","executionInfo":{"status":"aborted","timestamp":1771357432132,"user_tz":-180,"elapsed":5,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.ToPILImage(),      # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©\n","    transforms.Resize((224,224)), # ØªÙƒØ¨ÙŠØ± Ø§Ù„ØµÙˆØ±Ø©\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5sw9grURTu-","executionInfo":{"status":"aborted","timestamp":1771357432140,"user_tz":-180,"elapsed":113758,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["test_dataset = PneumoniaMNIST(split='test', download=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpxUjqUqRZQM","executionInfo":{"status":"aborted","timestamp":1771357432147,"user_tz":-180,"elapsed":113745,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["# Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ ØµÙˆØ±Ø© (Ù…Ø«Ù„Ø§Ù‹ Ø±Ù‚Ù… 5)\n","img, label = test_dataset[9]\n","\n","# ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ RGB (Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¨Ø§Ù„ÙØ¹Ù„)\n","if img.mode != 'RGB':\n","    image = img.convert(\"RGB\")\n","else:\n","    image = img\n","\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWBT5RD8SCMg","executionInfo":{"status":"aborted","timestamp":1771357432156,"user_tz":-180,"elapsed":113734,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ Ø­Ø§Ù„Ø© Pneumonia\n","for i in range(len(test_dataset)):\n","    img, label = test_dataset[i]\n","    if label == 1:   # 1 = Pneumonia\n","        # img Ù‡ÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ PIL ImageØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©\n","        image = img\n","        print(\"Pneumonia case index:\", i)\n","        break\n","\n","# ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙˆØ¶Ø¹ ÙˆØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ù‡ RGB\n","if image.mode != 'RGB':\n","    image = image.convert(\"RGB\")\n","\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJxW1vPBVj4U","executionInfo":{"status":"aborted","timestamp":1771357432162,"user_tz":-180,"elapsed":113710,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n","print(f\"Model type: {type(model).__name__}\")\n","print(f\"Processor type: {type(processor).__name__}\")\n","\n","# ØªØ¬Ø±Ø¨Ø© ØµÙŠØº Ù…Ø®ØªÙ„ÙØ©\n","prompts = [\n","    \"USER: You are a radiologist. Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\\nASSISTANT:\",\n","    \"USER: <image>\\nYou are a radiologist. Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\\nASSISTANT:\",\n","    \"Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\"\n","]\n","\n","for i, prompt in enumerate(prompts):\n","    try:\n","        print(f\"\\nTrying prompt {i+1}: {prompt[:50]}...\")\n","        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n","\n","        # Ø¥Ø°Ø§ Ù†Ø¬Ø­ØªØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§\n","        outputs = model.generate(**inputs, max_new_tokens=200)\n","        response = processor.decode(outputs[0], skip_special_tokens=True)\n","        print(\"Success! Response:\", response)\n","        break\n","    except Exception as e:\n","        print(f\"Failed with error: {type(e).__name__}\")\n","        continue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qbXneDJXt1x","executionInfo":{"status":"aborted","timestamp":1771357432169,"user_tz":-180,"elapsed":113699,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["# Ø¨Ø¹Ø¶ Ù†Ù…Ø§Ø°Ø¬ LLaVA ØªØªØ·Ù„Ø¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ©\n","try:\n","    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n","    from torchvision import transforms\n","\n","    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ tensor Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±\n","    if isinstance(image, PIL.Image.Image):\n","        transform = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","        ])\n","        image_tensor = transform(image).unsqueeze(0).to(model.device)\n","    else:\n","        image_tensor = image\n","\n","    # ØµÙŠØºØ© Ø®Ø§ØµØ© Ù„Ù€ LLaVA\n","    prompt_llava = \"USER: <image>\\nYou are a radiologist. Analyze this chest X-ray and write a short medical report. State if pneumonia is suspected.\\nASSISTANT:\"\n","\n","    # Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø®ØªÙ„ÙØ©\n","    inputs = processor(\n","        text=prompt_llava,\n","        images=image_tensor,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Ø§Ù†Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n","    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","\n","    outputs = model.generate(**inputs, max_new_tokens=200)\n","    response = processor.decode(outputs[0], skip_special_tokens=True)\n","    print(response)\n","\n","except Exception as e:\n","    print(f\"LLaVA format failed: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Tm-2My9YOrM","executionInfo":{"status":"aborted","timestamp":1771357432177,"user_tz":-180,"elapsed":113695,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"outputs":[],"source":["generate_ids = model.generate(\n","    **inputs,\n","    max_new_tokens=150,\n","    do_sample=True,\n","    temperature=0.2\n",")\n","\n","output = processor.decode(generate_ids[0], skip_special_tokens=True)\n","print(output)"]},{"cell_type":"code","source":["!pip install reportlab"],"metadata":{"id":"fU3aZq_h5gAV","executionInfo":{"status":"aborted","timestamp":1771357432184,"user_tz":-180,"elapsed":113680,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","import time\n","import random\n","from datetime import datetime\n","from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n","from reportlab.lib.styles import getSampleStyleSheet\n","from reportlab.lib.pagesizes import letter\n","import uuid\n","\n","# =====================================\n","# 1ï¸âƒ£ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±\n","# =====================================\n","REPORTS_DATABASE = {\n","    \"Normal\": \"\"\"\n","### ğŸ©º FINDINGS\n","- Lungs are clear with no focal consolidation.\n","- Cardiomediastinal silhouette within normal limits.\n","- No pleural effusion or pneumothorax detected.\n","\n","### âœ… IMPRESSION\n","No radiographic evidence of active cardiopulmonary disease.\n","\"\"\",\n","    \"Pneumonia\": \"\"\"\n","### ğŸ©º FINDINGS\n","- Patchy consolidation in the right lower lobe.\n","- Air bronchograms present.\n","- Mild pleural thickening observed.\n","\n","### âš ï¸ IMPRESSION\n","Radiographic findings are highly suggestive of Pneumonia.\n","Clinical correlation recommended.\n","\"\"\"\n","}\n","\n","# =====================================\n","# 2ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ PDF\n","# =====================================\n","def create_pdf(diagnosis, confidence, report_text):\n","    file_path = f\"/content/report_{uuid.uuid4().hex}.pdf\"\n","    doc = SimpleDocTemplate(file_path, pagesize=letter)\n","    styles = getSampleStyleSheet()\n","    elements = []\n","\n","    elements.append(Paragraph(\"<b>AI Radiology Report</b>\", styles[\"Title\"]))\n","    elements.append(Spacer(1, 12))\n","    elements.append(Paragraph(f\"Diagnosis: {diagnosis}\", styles[\"Normal\"]))\n","    elements.append(Paragraph(f\"Confidence: {confidence:.2f}%\", styles[\"Normal\"]))\n","    elements.append(Spacer(1, 12))\n","    elements.append(Paragraph(report_text.replace(\"\\n\",\"<br/>\"), styles[\"Normal\"]))\n","\n","    doc.build(elements)\n","    return file_path\n","\n","# =====================================\n","# 3ï¸âƒ£ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„\n","# =====================================\n","def analyze_and_report(img):\n","    if img is None:\n","        return gr.update(value=\"âš ï¸ Please upload an image first.\"), \"\", None\n","\n","    time.sleep(1.5)\n","\n","    diagnosis = random.choice([\"Normal\", \"Pneumonia\"])\n","    confidence = random.uniform(92.5, 99.7)\n","    report = REPORTS_DATABASE[diagnosis]\n","\n","    color = \"#16a085\" if diagnosis == \"Normal\" else \"#e74c3c\"\n","\n","    status_html = f\"\"\"\n","    <div style=\"\n","        background: linear-gradient(135deg, {color}, #2c3e50);\n","        padding: 25px;\n","        border-radius: 15px;\n","        color: white;\n","        text-align: center;\n","        box-shadow: 0 8px 20px rgba(0,0,0,0.15);\n","    \">\n","        <h2 style=\"margin:0;\">Diagnosis: {diagnosis}</h2>\n","        <p style=\"margin-top:10px;font-size:18px;\">\n","            AI Confidence: {confidence:.2f}%\n","        </p>\n","        <p style=\"font-size:12px;opacity:0.8;\">\n","            {datetime.now().strftime(\"%Y-%m-%d %H:%M\")}\n","        </p>\n","    </div>\n","    \"\"\"\n","\n","    pdf_path = create_pdf(diagnosis, confidence, report)\n","\n","    return status_html, report, pdf_path\n","\n","\n","# =====================================\n","# 4ï¸âƒ£ ØªØµÙ…ÙŠÙ… ÙˆØ§Ø¬Ù‡Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©\n","# =====================================\n","with gr.Blocks(\n","    theme=gr.themes.Soft(\n","        primary_hue=\"blue\",\n","        font=[gr.themes.GoogleFont(\"Inter\"), \"sans-serif\"]\n","    ),\n","    css=\"\"\"\n","    .gradio-container {\n","        max-width: 1100px !important;\n","        margin: auto;\n","    }\n","    \"\"\"\n",") as demo:\n","\n","    gr.Markdown(\"\"\"\n","    # ğŸ¥ Vision-Health AI\n","    ### Advanced Pneumonia Detection & Automated Clinical Reporting\n","    ---\n","    \"\"\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=1):\n","            input_image = gr.Image(\n","                type=\"pil\",\n","                label=\"Upload Chest X-ray\",\n","                height=350\n","            )\n","\n","            analyze_btn = gr.Button(\n","                \"ğŸš€ Run Full AI Analysis\",\n","                variant=\"primary\"\n","            )\n","\n","        with gr.Column(scale=2):\n","            status_output = gr.HTML(\n","                \"<div style='padding:40px;border:2px dashed #ccc;border-radius:15px;text-align:center;color:#888;'>Awaiting Image Upload...</div>\"\n","            )\n","\n","            report_output = gr.Markdown()\n","\n","            pdf_output = gr.File(label=\"ğŸ“¥ Download Official PDF Report\")\n","\n","    analyze_btn.click(\n","        analyze_and_report,\n","        inputs=input_image,\n","        outputs=[status_output, report_output, pdf_output]\n","    )\n","\n","    gr.Markdown(\"\"\"\n","    ---\n","    âš ï¸ This AI tool is for research purposes only and not a substitute for professional medical diagnosis.\n","    \"\"\")\n","\n","# =====================================\n","# 5ï¸âƒ£ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©\n","# =====================================\n","demo.launch(share=True)\n"],"metadata":{"id":"yf-ph0uV5ulS","executionInfo":{"status":"aborted","timestamp":1771357432239,"user_tz":-180,"elapsed":113708,"user":{"displayName":"ÙØ±Ø§Ø³","userId":"06494686585529629476"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1cXuMjXhYHjA-AbKRYGDjibTVA2047YSy","timestamp":1771315922109},{"file_id":"11JL9gQCUEeYGi4NQiGrTGOG8AlDTa8LC","timestamp":1771261985125}],"authorship_tag":"ABX9TyNegnxfShuogr1UvS654T0D"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8f93a62897cb4fca8b44f313d29f73bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b6bd0a30ef24c17817a4e1da8608b8e","IPY_MODEL_1f60850a301c4809b9451e7b5a3a33c3","IPY_MODEL_08ad5a410312496b9c1f518a208544ac"],"layout":"IPY_MODEL_4d6c2fc28d854f288b433a41ef3eb72d"}},"5b6bd0a30ef24c17817a4e1da8608b8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2712e63f74904d3e83414a1c44d334e7","placeholder":"â€‹","style":"IPY_MODEL_1ccc4e4568ee400caaeb54d586c76f9c","value":"model.safetensors:â€‡100%"}},"1f60850a301c4809b9451e7b5a3a33c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae364c2f2f7c434182cb9a352b91e2d9","max":22883348,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8481e09ffee041c484bab4cba9d97e5e","value":22883348}},"08ad5a410312496b9c1f518a208544ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_671be8262c904f6a82985d82e3b84e4f","placeholder":"â€‹","style":"IPY_MODEL_66e1bf6b33c044a7957ac9af777422ee","value":"â€‡22.9M/22.9Mâ€‡[00:01&lt;00:00,â€‡21.1MB/s]"}},"4d6c2fc28d854f288b433a41ef3eb72d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2712e63f74904d3e83414a1c44d334e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ccc4e4568ee400caaeb54d586c76f9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae364c2f2f7c434182cb9a352b91e2d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8481e09ffee041c484bab4cba9d97e5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"671be8262c904f6a82985d82e3b84e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66e1bf6b33c044a7957ac9af777422ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}