import torch
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# ============================================
# 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
# ============================================

def check_requirements():
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… Using device: {device}")
    print(f"âœ… PyTorch version: {torch.__version__}")

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ model Ùˆ test_loader
    global model, test_loader
    try:
        print(f"âœ… Model type: {type(model).__name__}")
        print(f"âœ… Test loader batches: {len(test_loader)}")
    except NameError:
        print("âŒ Error: model or test_loader not defined!")
        return False

    return True

# ============================================
# 2. Ø¯Ø§Ù„Ø© Ù„ÙØ­Øµ ÙˆØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±
# ============================================

def get_image_size_from_loader(test_loader):
    """ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± ÙÙŠ test_loader"""
    for images, _ in test_loader:
        return images.shape[2:]  # (height, width)
    return None

def fix_image_size(image, target_size=(224, 224)):
    """ØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
    if image.shape[1:] != target_size:
        resize = transforms.Resize(target_size)
        return resize(image)
    return image

# ============================================
# 3. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ‚ÙŠÙŠÙ… (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø±Ù‚Ø§Ù…)
# ============================================

def evaluate_model_safe(model, test_loader):
    """
    Ø¯Ø§Ù„Ø© ØªÙ‚ÙŠÙŠÙ… Ù…ØªÙƒØ§Ù…Ù„Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ø£Ø­Ø¬Ø§Ù… Ø§Ù„ØµÙˆØ±
    """
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±
    sample_shape = None
    for images, _ in test_loader:
        sample_shape = images.shape
        print(f"ğŸ“Š Detected image size: {sample_shape[2]}x{sample_shape[3]}")
        break

    # Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    expected_size = 224  # ViT expects 224x224

    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø¬Ù… ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ¹Ø¯ÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
    if sample_shape and (sample_shape[2] != expected_size or sample_shape[3] != expected_size):
        print(f"âš ï¸  Image size mismatch: Got {sample_shape[2]}x{sample_shape[3]}, expected {expected_size}x{expected_size}")
        print("ğŸ”„ Automatically resizing images during evaluation...")

        # Ø¥Ø¶Ø§ÙØ© resize transform
        resize_transform = transforms.Resize((expected_size, expected_size))

        # ØªØ¹Ø¯ÙŠÙ„ test_loader Ù…Ø¤Ù‚ØªØ§Ù‹
        original_collate = test_loader.collate_fn

        def collate_with_resize(batch):
            images = []
            labels = []
            for img, label in batch:
                if img.shape[1:] != (expected_size, expected_size):
                    img = resize_transform(img)
                images.append(img)
                labels.append(label)
            return torch.stack(images), torch.tensor(labels)

        test_loader.collate_fn = collate_with_resize
        print("âœ… Automatic resizing enabled!")

    model.eval()
    all_preds = []
    all_labels = []

    print("\nğŸš€ Starting evaluation process...")
    print(f"ğŸ“¦ Number of batches: {len(test_loader)}")

    with torch.no_grad():
        for batch_idx, (images, labels) in enumerate(test_loader):
            try:
                images = images.to(device)

                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
                if images.shape[2:] != (expected_size, expected_size):
                    print(f"âš ï¸  Batch {batch_idx}: Unexpected shape {images.shape}, resizing...")
                    images = torch.stack([fix_image_size(img, (expected_size, expected_size)) for img in images])
                    images = images.to(device)

                # Forward pass
                outputs = model(images)

                # Handle Hugging Face output format
                if hasattr(outputs, 'logits'):
                    logits = outputs.logits
                else:
                    logits = outputs

                # Get predictions
                preds = torch.argmax(logits, dim=1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                # Progress update
                if (batch_idx + 1) % max(1, len(test_loader)//5) == 0:
                    print(f"âœ… Processed {batch_idx + 1}/{len(test_loader)} batches")

            except Exception as e:
                print(f"âŒ Error in batch {batch_idx}: {e}")
                continue

    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy
    all_labels = np.array(all_labels).flatten()
    all_preds = np.array(all_preds).flatten()

    print(f"\nğŸ“Š Total samples evaluated: {len(all_labels)}")
    print(f"ğŸ“Š Class distribution: {np.unique(all_labels, return_counts=True)}")

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
    acc = accuracy_score(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)

    # Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„
    target_names = ['Normal', 'Pneumonia']
    report = classification_report(all_labels, all_preds, target_names=target_names)

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\n" + "="*50)
    print(f"ğŸ¯ OVERALL ACCURACY: {acc:.4f} ({acc*100:.2f}%)")
    print("="*50)
    print("\nğŸ“‹ CLASSIFICATION REPORT:")
    print(report)

    print("\nğŸ“Š CONFUSION MATRIX:")
    print(cm)

    # ============================================
    # Ø±Ø³Ù… Confusion Matrix Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±)
    # ============================================
    plt.figure(figsize=(12, 10))

    # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ø³ØªØ®Ø¯Ø§Ù… annot=True ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ù†ØµÙˆØµ ÙŠØ¯ÙˆÙŠØ©)
    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                     xticklabels=target_names,
                     yticklabels=target_names,
                     annot_kws={'size': 16, 'weight': 'bold'},
                     cbar_kws={'label': 'Count'})

    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=14, fontweight='bold')
    plt.title('Confusion Matrix - ViT Model', fontsize=16, fontweight='bold')

    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ© ÙÙŠ Ø®Ø§Ù†Ø© Ù…Ù†ÙØµÙ„Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            if cm[i, j] > 0:
                percentage = cm[i, j] / np.sum(cm[i, :]) * 100
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ø£Ø³ÙÙ„ Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                       ha='center', va='center', color='black', fontsize=10)

    plt.tight_layout()
    plt.show()

    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    if cm.size == 4:
        tn, fp, fn, tp = cm.ravel()

        print("\nğŸ“ˆ DETAILED METRICS:")
        print(f"   â€¢ True Positives: {tp}")
        print(f"   â€¢ True Negatives: {tn}")
        print(f"   â€¢ False Positives: {fp}")
        print(f"   â€¢ False Negatives: {fn}")

        if tp + fn > 0:
            sensitivity = tp / (tp + fn)
            print(f"   â€¢ Sensitivity (Recall): {sensitivity:.4f}")

        if tn + fp > 0:
            specificity = tn / (tn + fp)
            print(f"   â€¢ Specificity: {specificity:.4f}")

        # Precision and F1-score
        if tp + fp > 0:
            precision = tp / (tp + fp)
            print(f"   â€¢ Precision: {precision:.4f}")

        if precision + sensitivity > 0:
            f1 = 2 * (precision * sensitivity) / (precision + sensitivity)
            print(f"   â€¢ F1-Score: {f1:.4f}")

    return acc, cm

# ============================================
# 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ============================================

def validate_model_and_data():
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"""

    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ model
    try:
        model
    except NameError:
        print("âŒ ERROR: 'model' is not defined!")
        print("Please define your model first.")
        return False

    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ test_loader
    try:
        test_loader
    except NameError:
        print("âŒ ERROR: 'test_loader' is not defined!")
        print("Please define your test_loader first.")
        return False

    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    model.eval()

    # ÙØ­Øµ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        for images, labels in test_loader:
            print(f"âœ… Sample batch - Images: {images.shape}, Labels: {labels.shape}")
            print(f"âœ… Label values: {torch.unique(labels)}")
            break
    except Exception as e:
        print(f"âŒ Error accessing test_loader: {e}")
        return False

    return True

# ============================================
# 5. Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
# ============================================

print("="*60)
print("ğŸ” VIT MODEL EVALUATION SYSTEM")
print("="*60)

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
if validate_model_and_data():
    print("\nâœ… Validation passed. Starting evaluation...\n")

    # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    try:
        acc, cm = evaluate_model_safe(model, test_loader)
        print("\nâœ¨ Evaluation completed successfully!")
    except Exception as e:
        print(f"\nâŒ Unexpected error during evaluation: {e}")
        print("\nğŸ› ï¸  Troubleshooting tips:")
        print("   1. Check if model is correctly loaded")
        print("   2. Verify test_loader contains valid data")
        print("   3. Ensure GPU memory is sufficient")
        print("   4. Try reducing batch size if memory issues")
else:
    print("\nâŒ Validation failed. Please check the errors above.")
